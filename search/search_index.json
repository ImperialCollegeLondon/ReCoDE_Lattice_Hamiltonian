{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#modelling-organic-crystals-using-a-lattice-hamiltonian","title":"Modelling Organic Crystals Using a Lattice Hamiltonian","text":""},{"location":"#description","title":"Description","text":"<p>In this project, we use a simplified model of an organic crystal to calculate the system's excited states under illumination and their populations. We then investigate how changing the input parameters of the model changes the nature of the excited states. An example is shown in the GIF below where we demonstrate how applying an electric field changes the energies and populations of the eigenstates. </p> <p></p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Techniques to speed up <code>for</code> loops</li> <li>How to run <code>for</code> loops in parallel uing the <code>multiprocessing</code> package</li> <li>How to plot heatmaps using the <code>seaborn</code> package</li> </ul>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#academic","title":"Academic","text":"<ul> <li>Intermediate-level python ability</li> <li>Strong, undergraduate-level understanding of quantum mechanics (recommended)</li> <li>Basic familiarity with solid state physics (recommended)</li> </ul>"},{"location":"#system","title":"System","text":"<ul> <li>See the pyproject.toml file</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Start by reading through sections 1-4 which describe the physics underlying this exemplar and the structure of the code. </p> <p>Once you have been through this, you can work through the next four sections. In the first of these, we walk you through how to use the code and investigate how the eigenstates of the system change when we change the strength of the coupling between lattice sites. The next three sections focus in detail on short extracts from the code which are relevant to the learning outcomes of this exemplar. </p> <p>There are lots of things which the code can be used to do that aren't explicitly covered here! If you want a challenge, try to figure our how you could recreate the GIF shown above. Considering the eigenstates with the highest probability of being occupied, what changes about their electron-hole separation as the electric field strength is increased?</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"01_ModelDescription/","title":"Introduction","text":"<p>In this ReCoDe exemplar, we use quantum mechanics to describe the photoexcited states of a molecular crystal and explore how the properties of these states, such as their energies, vary as we alter different input parameters. To do this, we model the system using a tight-binding Hamiltonian, the eigenstates of which correspond to the system's photoexcited states. In the Hamiltonian, we account for the following interactions between the excited states:</p> <ul> <li>The electrostatic interaction between spatially separated electrons and holes.</li> <li>The electronic coupling between molecules which allows electons and holes to move from one lattice site to an adjacent lattice site.</li> <li>The energetic stabilisation of electrons and holes which are located on the same lattice site (i.e., exciton formation).</li> <li>The interaction between spatially separated excitonic states (i.e., energy transfer).</li> </ul> <p>Below, we describe the construction of the Hamiltonian in greater detail and we explain how we include each of these interactions. </p>"},{"location":"01_ModelDescription/#constructing-the-hamiltonian","title":"Constructing the Hamiltonian","text":"<p>To model the system, we consider a 2D lattice in which each site represents a molecule. When a molecule is photoexcited, one electron will move from the highest occupied molecular orbital (HOMO) to the lowest unoccupied molecular orbital (LUMO). Due to the low dielectric constant of organic materials, the resulting electron-hole pair will form a tightly bound state called a Frenkel exciton. This excitonic state is energetically stabilised relative to two free charges in the same material, meaning that its energy is lower than the bandgap of the material by an amount termed the exciton binding energy. For the electron and hole to spatially separate from one another and become free charges, they must overcome this binding energy. Typically, this is not a spontaneous process and requires, for example, the applicaiton of an electric field. </p> <p>To account for the different properties of the system's exctied states, our model contains two types of basis state: excitons, when the electron and hole occupy the same lattice site and electron-hole pairs, when the electron and hole are located on different lattice sites. The energy of excitonic states, , is given by </p> <p> </p> <p>In which  and  and the energy levels of the LUMO and HOMO, respectively and  is the exciton binding energy. The energy of electron-hole pairs is given by</p> <p> </p> <p>where  is the vector connecting the electron-hole pair, and the second term represents the electrostatic attraction between them, which we have modelled using the Mataga potential. The Mataga potenial is parameterised by the two variables  and . The model can also include the effects of uniform electric field, , by altering the energies of electron-hole basis elements by an amount .</p> <p>These energies form the diagonal elements of the Hamiltonian. To simulate the disorder commonly found in the site energies of organic crystals, the model includes a Gaussian disorder term, which is added to the diagonal elements of the Hamiltonian. </p> <p>In the off diagonal elements, we consider interactions between the basis states. Each exciton is weakly coupled to other excitons in the lattice, and we model this using a dipole-dipole interaction which takes the functional form:  </p> <p> </p> <p>where  and  are the two parameters which characterise the strength of the dipole-dipole interaction,  is the spacing between lattice sites and  is a parameter which controls the maximum separation which a pair of excitons can have for their couipling to be included in the Hamiltonian. Typically, we set this to  since the  dependence of  means that the coupling between exciton pairs is generally negligible for pairs which are further apart than diagonal neighbours</p> <p>In addition to this coupling between excitonic states, it is also possible for the electron (hole) to hop to a neighbouring lattice site, leaving the initial molecule with a positive (negative) charge and giving one of its neighbours a negative (positive) charge. The size of the coupling describing this interaction depends on the wavefunction overlap between the molecules involved, which typically decays exponentially with distance. Thus, we assume that this type of interaction can only take place between directly adjacent lattice sites and it is parameterised by the electronic coupling  (). </p> <p>Putting these parts together, we can write the Hamiltonian describing the electronic states as</p> <p> </p> <p> </p> <p>where basis states indexed with k are excitonic in character, while those indexed using i and j describe electron-hole pairs. The i index refers to the lattice site on which the electron is located and the j index to that on which the hole is localised. We then diagonalise this Hamiltonian to find the eigenstates and energy levels of the system.</p> <p><sub> We note that this form of the dipole-dipole interaction is equivalent to assuming that the transition dipole moments of the basis states are all oriented parallel to one another.<sub>"},{"location":"01_ModelDescription/#coupling-to-the-environment","title":"Coupling to the Environment","text":"<p>While the Hamiltonian described in the previous section calculates the electronic eigenstates of the system, it neglects their coupling to the environment. As the crystal is at a non-zero temperature, the molecules making up the crystal will be in motion. Due to the relatively floppy nature of organic molecules, this thermal motion can cause their conformation (i.e., the arrangement of the atoms in each molecule) to fluctuate significantly. These fluctuations in the molecular conformations affect the energies of the (excited) electronic states and can drive transitions between them, as discussed in Section Two.</p> <p>In organic crystals, the dominant thermally activated motions are typically vibrations of the intramolecular bonds of the individual molecules in the lattice (e.g., C=C or C-H bonds). Each distinct type of vibration for a given molecule is referred to as a 'mode'. The energies of these vibrational modes and the strength with which each molecule couples to them can be summarised by a single function called the spectral density function, . In this function,  refers to the energy of the vibrational mode and the magnitude of  is proportional to the strength of the coupling. The spectral density function used in this exemplar is shown in Figure One, below, and is defined mathematically in Appendix Two. </p> <p></p> <p>Figure One: The spectral density function used in this exemplar. The shaded region indicates the portion of the spectral density function which contributes to the outer reorganisation energy, a parameter which is used in the calculation of the excitons' decay rate if this is not assumed to be constant (see Appendix One).</p>"},{"location":"02_FindingSteadyStatePopulations/","title":"Calculating Steady-State Occupations","text":"<p>Having found the eigenstates of the electronic Hamiltonian, we now want to find the relative populations of these eigenstates under illumination. To do this, we need to solve the rate equations of the system which are </p> <p> </p> <p>in which  is the population of the eigenstate  and where we have defined the following rates:</p> <ul> <li>The rate at which the eigenstate  is generated by the illuminaton, .</li> <li>The rate at which the population of the eigenstate  decreases due to population transfer into the eigenstate , .</li> <li>The rate at which the population of the eigenstate  increases due to population transfer from the eigenstate , .</li> <li>The rates at which the eigenstate  returns to the ground state, . </li> </ul> <p>In this section, we will describe how these rates are calculated within this exemplar, though we note that this part of the model can be adjusted to accommodate different levels of theory. </p>"},{"location":"02_FindingSteadyStatePopulations/#generation-rates","title":"Generation Rates","text":"<p>To calculate the generation rate into a given eigenstate, we assign a generation probability to excitonic basis states, which is determined by the <code>transition_dipole_ex</code> parameter (). </p> <p> </p> <p>where the summation over  evaulates the contribution to the eigenstate from excitonic basis states, . This can be thought of as a way of characterising how much of the eigenstate is made up from excitonic basis states. A value close to one implies that the eigenstate is an exciton, while a value close to zero indicates that the eigenstate has charge transfer character (i.e., the electron and hole are on different molecules). </p>"},{"location":"02_FindingSteadyStatePopulations/#recombination-rates","title":"Recombination Rates","text":"<p>In our exemplar, the recombination of eigenstates to the ground state is assumed to be dominated by non-radiative recombination. We model this in one of two ways, depending upon the value of the <code>const_recombination</code> argument of the <code>Parameters</code> class. If this is set to True, a recombination rate is assigned to excitonic basis states using the <code>krec_ex</code> parameter. Then, the total recomination rate of the eigenstate  is calculated as </p> <p> </p> <p>Alternatively, if the <code>const_recombination</code> argument of the <code>Parameters</code> class is set to False, the decay rates are calculated using a version of Fermi's Golden Rule which has been adapted to describe organic molecules. This is described in further detail in Appendix One. </p>"},{"location":"02_FindingSteadyStatePopulations/#rates-of-population-transfer","title":"Rates of Population Transfer","text":"<p>The rates of population transfer, , are calculated using secular Redfield theory. While the derivation of these rates is mathematically involved (see e.g., ref 1), the equation used to calculate them can be written in a relatively simple form</p> <p> </p> <p>where the function  is called the correlation function and is defined in terms of the Bose-Einstein occupation function, , and the spectral density function, , as follows</p> <p> </p> <p>Note that, for each value of , you must sum over all basis states. For a  square lattice, the total number of eigenstates is  and so the number of rates which must be calculated is . Consequaently, the calculation of the  is typically the most time consuming step of the simulation. </p>"},{"location":"02_FindingSteadyStatePopulations/#references","title":"References","text":"<ol> <li>Dynamics of Isolated and Open Quantum Systems in Charge and Energy Transfer Dynamics in Molecular Systems pg 67\u2013190 (John Wiley &amp; Sons, Ltd, 2011). doi:10.1002/9783527633791.ch3.</li> </ol>"},{"location":"03_CalculatingEigenstateProperties/","title":"Calculating Properties of the Eigenstates","text":"<p>The eigenstates of the system are found by solving the eigenvalue problem</p> <p> </p> <p>Each excited state, , is characterised by an energy, , and eigenvector</p> <p> </p> <p>The electron-hole separation of the state is calculated as</p> <p> </p> <p>where  is the vector describing the position of the basis state  in the lattice. Other eigenstate properties, such as their decay rates, can be calculated as expectation values in a similar manner. </p> <p>The excitonic and charge transfer character of each eigenstate state can be evaluated using the contributions from excitonic  and charge transfer states  basis elements, respectively, using the expressions</p> <p> </p> <p> </p> <p>The calculated excited states will be delocalised over the whole basis, where the degree of delocalization is given by the inverse participation ratio, which is generally defined as</p> <p> </p> <p>For a given eigenstate, the inverse participation ratio of the exciton, electron and hole is calculated by first defining a reduced wavefunction for each of these species. For example, in the case of the electron</p> <p> </p> <p>where the prefactor ensures that the new wavefunction is properly normalised. This wavefunction is then substituted into the definition of the inverse participation ratio to get </p> <p> </p> <p>The same reasoning can be used to find the IPR of the exciton and hole contributions to the eigenstate's wavefunction i.e., </p> <p> </p> <p> </p>"},{"location":"04_OverviewOfCode/","title":"Summary","text":"<p>The structure of the <code>lattice_hamiltonian</code> module is illustrated in the above schematic. The core of the code is the <code>Lattice</code> class in the <code>lattice.py</code> file. This class contains all the methods needed to construct the Hamiltonian (<code>generate_uniform</code> and <code>build_ham</code>), find its eigenstates (<code>states_from_ham</code>) and calculate their steady-state populations (<code>get_rates_mat</code> and <code>solve_steady</code>). The methods <code>states_from_ham</code> and <code>get_rates_mat</code> call on helper functions from the files <code>Recombination.py</code> and <code>Redfield.py</code>, respectively, though <code>Recombination.py</code> is only used if <code>const_recombination</code> is set to <code>False</code> (see Section Three). </p> <p>In addition to the <code>Lattice</code> class, the <code>lattice.py</code> file also contains the <code>Parameter</code> and <code>Site</code> dataclasses. The former contains the values of global variables, such as temperature, which are used in all the other functions and the latter contains the properties of each individual site in the lattice and is called by the <code>generate_uniform</code> method. These sites are then used to define the basis set in which the Hamiltonian is built by the <code>build_ham</code> method. </p> <p>To see an example of how the lattice class can be instantiated and used, refer to the <code>RunLattice.py</code> file. If you would like to try running the code, you can use the <code>WorkedExample.ipynb</code> file to see how changing the electronic coupling affects the energies and distributions of the system's eigenstates. </p>"},{"location":"05-WorkedExample/","title":"Worked Example","text":"<p>Note that you will need to install the lattice_hamiltonian module before you can run this notebook. You can do this by running python -m pip install from the command line after navigating to the directory where you have saved the ReCoDe_Lattice_Hamiltonian project.</p> <p>Run the cell below to import the modules needed to run the lattice Hamiltonian. We also import some other, basic Python modules which are used for saving the outputs of this notebook.</p> In\u00a0[1]: Copied! <pre>import os\nimport warnings\nfrom datetime import datetime\n\nfrom lattice_hamiltonian.lattice_plots import plot_energies, plot_state_distribution\nfrom lattice_hamiltonian.run_lattice import sweep_parameter\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n</pre> import os import warnings from datetime import datetime  from lattice_hamiltonian.lattice_plots import plot_energies, plot_state_distribution from lattice_hamiltonian.run_lattice import sweep_parameter  warnings.filterwarnings(\"ignore\", category=RuntimeWarning) <p>Use the next cell to define the file where you want to save any output plots. By default, plots will be saved in the 'plots' folder under the current working directory, and labelled by today's date.</p> In\u00a0[2]: Copied! <pre>pwd = os.getcwd()\nnow = datetime.now()\nsave_path = pwd + \"/plots/\" + now.strftime(\"%d%b%Y-1/\")\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\n</pre> pwd = os.getcwd() now = datetime.now() save_path = pwd + \"/plots/\" + now.strftime(\"%d%b%Y-1/\") if not os.path.exists(save_path):     os.makedirs(save_path) <p>In the cell below, we run the code for a range of values of the electronic coupling parameter, <code>t0</code>. This parameter determines how strongly neighbouring lattice sites interact with one another, and thus the extent to which the eigenstates are localised on single lattice sites (small <code>t0</code>) or spread over the whole lattice (large <code>t0</code>).</p> <p>To run the parameter sweep, we use the <code>sweep_parameter</code> function from <code>RunLattice.py</code>. This function calls solves the Hamiltonian for each value of the <code>parameter_to_vary</code> in <code>parameter_array</code>. All the other input parameters are held constant at the values defined in the <code>parameter_dict</code>.</p> <p>The output of <code>sweep_parameter</code> is a dictionary containing the solutions for each value of <code>t0</code>. Each solution is an instance of the <code>Lattice</code> class, from which you can access information about the eigenstates. This information is stored in a dataframe which is an attribute of the <code>Lattice</code> class and can be accessed as follows: <code>lattice_dict[i].states</code>, where <code>i &lt; len(parameter_array)</code>.</p> <p>We note that we have set <code>size = 5</code> in this file to ensure that the code runs quickly (it should take ~30 seconds). Larger lattices can be used (up to <code>size = 10</code>), but the time taken to solve the Hamiltonian and find the steady-state populations of the eigenstates increases rapidly with the size of the lattice, so it could take a while to complete the calculations.</p> In\u00a0[3]: Copied! <pre># This should be a string - see the RunLattice.py file for all the possible parameters\nparameter_to_vary = \"t0\"\n# A list containing the values you want parameter_to_vary to take\nparameter_array = [2e-3, 4e-3, 6e-3, 8e-3, 10e-3]\n# A dictionary containing the value of all the parameters which are being held constant\nparameter_dict = {\n    \"size\": 5,\n    \"e_singlet\": 1.4,\n    \"d0\": 5e-3,\n    \"r0d\": 0.1,\n    \"const_recombination\": True,\n    \"krec_ex\": 1e9,\n    \"j0\": 1.5,\n    \"r0j\": 0.1,\n    \"F\": 0,\n    \"disorder_site_ene\": 50e-3,\n}\n\nlattice_dict = sweep_parameter(parameter_to_vary, parameter_array, parameter_dict)\n</pre> # This should be a string - see the RunLattice.py file for all the possible parameters parameter_to_vary = \"t0\" # A list containing the values you want parameter_to_vary to take parameter_array = [2e-3, 4e-3, 6e-3, 8e-3, 10e-3] # A dictionary containing the value of all the parameters which are being held constant parameter_dict = {     \"size\": 5,     \"e_singlet\": 1.4,     \"d0\": 5e-3,     \"r0d\": 0.1,     \"const_recombination\": True,     \"krec_ex\": 1e9,     \"j0\": 1.5,     \"r0j\": 0.1,     \"F\": 0,     \"disorder_site_ene\": 50e-3, }  lattice_dict = sweep_parameter(parameter_to_vary, parameter_array, parameter_dict) <p>As noted above, we can now investigate some properties of the eigenstates by viewing the <code>states</code> dataframe (we have dropped the columns containing arrays to improve readability):</p> In\u00a0[5]: Copied! <pre>lattice_dict[4].states.drop(['IPR','occupation_probability'], axis = 1).head(5)\n</pre> lattice_dict[4].states.drop(['IPR','occupation_probability'], axis = 1).head(5) Out[5]: state energies dis_eh ex_char transdip_ex krec_ex gen z 0 1 1.293656 0.062609 0.993762 0.993762 9.937616e+08 0.993762 9.616279e-09 1 2 1.314679 0.029000 0.997104 0.997104 9.971041e+08 0.997104 4.264775e-09 2 3 1.319829 0.042339 0.995778 0.995778 9.957776e+08 0.995778 3.498093e-09 3 4 1.339312 0.042081 0.995805 0.995805 9.958048e+08 0.995805 1.663087e-09 4 5 1.342775 0.068531 0.993172 0.993172 9.931720e+08 0.993172 1.455195e-09 <p>Although the dataframe is useful, it is hard to visualise the states in this format. To get a better idea of how the energies of the eigenstates depends upon the electron-hole seperation, $r_{\\mathrm{e-h}}$, we can use the <code>plot_energies</code> function. Run the cell below to use this function and see how the energies of the eigenstates depends on the electron-hole separation and the value of <code>t0</code>.</p> In\u00a0[5]: Copied! <pre># Set save = True if you want to save the figure\nsave = False\n# These labels will go in the legend of the figure\nlabels = [\"2 meV\", \"4 meV\", \"6 meV\", \"8 meV\", \"10 meV\"]\nplot_energies(lattice_dict, parameter_array, labels, parameter_to_vary, save, save_path)\n</pre> # Set save = True if you want to save the figure save = False # These labels will go in the legend of the figure labels = [\"2 meV\", \"4 meV\", \"6 meV\", \"8 meV\", \"10 meV\"] plot_energies(lattice_dict, parameter_array, labels, parameter_to_vary, save, save_path) <p>Looking at the plot above, we can see that there two groups of states: a cluster located at $r_{\\mathrm{e-h}}$ = 0 and a broad spread of states with greater values of $r_{\\mathrm{e-h}}$.</p> <p>The states at $r_{\\mathrm{e-h}}$ = 0 are the excitons (i.e., the elctron and hole occupy the same lattice site) and we can see that their distribution is not greatly affected by changing <code>t0</code>.</p> <p>The broad spread of states is due to states where the electron and hole are on different lattice sites. The energy of these states increases as $r_{\\mathrm{e-h}}$ increases because the electron and hole must do work to overcome their mutual electrostatic attraction. The plot shows that these states begin to take a more continuous range of values of $r_{\\mathrm{e-h}}$ as <code>t0</code> increases. This is due to their increasing delocalisation, as you can explore in the cells below.</p> <p>Run the cell below to see how the electron and hole probability density is distributed across the lattice for a given eigenstate. You can choose which solution to look at by varying the <code>which_lattice</code> parameter and you can select a state by varing the <code>which_state</code> parameter. By default, the eigenstates are ranked by their energy and so <code>which_state = 0</code> corresponds to the lowest energy state. What do you notice about the delocalisation of the states as you go from small <code>t0</code> (<code>which_lattice = 0</code>) to large <code>t0</code> (<code>which_lattice = 4</code>)?</p> In\u00a0[6]: Copied! <pre># Set save = True if you want to save the figure\nsave = False\n\nwhich_lattice = 2\nwhich_state = 500\nplot_state_distribution(\n    lattice_dict[which_lattice], which_state, save, save_path=save_path\n)\n</pre> # Set save = True if you want to save the figure save = False  which_lattice = 2 which_state = 500 plot_state_distribution(     lattice_dict[which_lattice], which_state, save, save_path=save_path )"},{"location":"05-WorkedExample/#worked-example","title":"Worked Example\u00b6","text":""},{"location":"06_SpeedingUp-for-Loops/","title":"Speeding up <code>for</code> loops: the <code>get_rate_mat</code> function","text":"<p>The purpose of the <code>get_rate_mat</code> function in <code>lattice.py</code> is to calculate the elements of the rate matrix which describes the rate of population transfer between the excited states of the lattice. Each element of the rate matrix takes the form </p> <p> </p> <p>as is described in greater detail here. Looking at this summation, we see that each term in the rate matrix requires a summation over all the basis states of the system (basis states are indexed using the latin alphabet, and eigenstates using the greek alphabet). For a  lattice, the number of basis states is . In addition to this, the rate matrix itself contains  elements as we calculate rates between every possible pair of eigenstates. Thus, the calculation of the rate matrix scales as  meaning that it limits the speed of the code for all but the smallest lattice sizes. This means that we need this function to run as quickly as possible in order to minimise the overall runtime of the code and, in this file, we will explore some of the techniques we have used to achieve this. </p>"},{"location":"06_SpeedingUp-for-Loops/#1-vectorize-functions","title":"1) Vectorize functions","text":"<p>Vectorizing functions is always a good first step to speed up python code. Simply put, a vectorized function is one which can perform an operation on multiple elements of an input array at once, rather than having to act on each element in turn. Many common operators in Python are already vectorized. For example, consider the following exponentiation: <pre><code>x = np.linspace(0,100,101)\ny = np.zeros(len(x))\nfor i in range(len(x)):\n  y[i] = x[i]*x[i]\n</code></pre> This can be easily replaced with  <pre><code>y = x**2\n</code></pre> In general, it is always good to try and use pre-exsisting functions from libraries such as numpy or scipy as these functions are typically vectorized and also use C to do much of the heavy lifting, making them much faster than equivalent functions written purely in Python. For example, in <code>get_rate_mat</code>, we use the <code>np.matmul</code> function (equivalent to the <code>@</code> operator) to perform the summation over all of the lattice's basis states.</p> <p>However, in some cases, there will not be a pre-existing function which does what you need and then it is best to write your own, vectorized function. We have done this to calculate the values of the correlation function ( in the expression for  above). Our functions are saved in the <code>redfield.py</code> file and we call them in <code>get_rate_mat</code> via the <code>correlation_function_real_part</code> function. However, to take advantage of the functions in <code>redfield.py</code>, we first need to make a numpy array containing the difference in energy between each possible pair of eigenstates (i.e., the ) which we do using...</p>"},{"location":"06_SpeedingUp-for-Loops/#2-generators","title":"2) Generators","text":"<p>To make a numpy array containing the values of , we combine the numpy function <code>np.fromiter</code> (see the documentation here) with a generator written using the same syntax as a list comprehension. A list comprehension is a way of rewriting <code>for</code> loops which is particularly useful if the logic involved in each iteration of the for loop is simple enough to be encapsulated in a single line. For example, the code: <pre><code>a = [1,2,3,4,5]\nb = []\nfor i in range(len(a)):\n  b.append(a[i]**2)\n</code></pre> can be rewritten as: <pre><code>a = [1,2,3,4,5]\nb_list_comp = [i**2 for i in a]\n</code></pre> which can then be turned into a generator as follows: <pre><code>b_generator = (i**2 for i in a)\n</code></pre> Generators are useful in situations when you want to perform an operation on a large number of values, but you don't want to store all the input values in the memory at once. </p> <p>In order to use our generator, we need to make it so that each element of the rate matrix is labelled by a single index, rather than needing to label both the row and the column (i.e., the values of  and  in ). Fortunately, numpy has a built in function (<code>np.tril_indices_from</code>, see the documentation) which can generate an iterable of the indices describing the elements making up a lower (or upper) triangular matrix. </p> <p>By combining these steps of vectorization and using generators, we achieved a significant speed up in the time taken to claculate the rate matrix, as we show in the plot below (the code used to generate this plot can be found in Appendix Three. Both the lines run (roughly) parallel to one another, which shows us that they scale in approximately the same way as lattice size increases. For all lattice sizes represented, the vectorized version is 3-4 times faster, which is a worthwhile speed-up.</p> <p></p>"},{"location":"07-Using-multiprocessing/","title":"Running for Loops in Parallel","text":"<p>In this file, we will look at how to use the <code>multiprocessing</code> package to run a for loop in parallel and speed up a parmeter sweep.</p> <p>First, run the below to import the modules needed for this workbook. Then, run the following cell to perform a sequential parameter sweep over the electronic coupling (<code>t0</code>) parameter. The code will print out how long this takes so that we have a benchmark to compare with the time taken for the parallelised parameter sweep.</p> In\u00a0[1]: Copied! <pre>import warnings\nfrom datetime import datetime\n\nfrom lattice_hamiltonian.run_lattice import single_parameter, sweep_parameter\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n</pre> import warnings from datetime import datetime  from lattice_hamiltonian.run_lattice import single_parameter, sweep_parameter  warnings.filterwarnings(\"ignore\", category=RuntimeWarning) In\u00a0[2]: Copied! <pre># This should be a string - see the RunLattice.py file for all the possible parameters\nparameter_to_vary = \"t0\"\n# A list containing the values you want parameter_to_vary to take\nparameter_array = [2e-3, 4e-3, 6e-3, 8e-3, 10e-3]\n# A dictionary containing user specified input variables.\n# Unspecified parameters will be set to default values.\nparameter_dict = {\"size\": 5}\n\nt_start = datetime.now()\nlattice_dict = sweep_parameter(parameter_to_vary, parameter_array, parameter_dict)\nt_end = datetime.now()\nprint(f\"Time taken: {t_end - t_start}\")\n</pre> # This should be a string - see the RunLattice.py file for all the possible parameters parameter_to_vary = \"t0\" # A list containing the values you want parameter_to_vary to take parameter_array = [2e-3, 4e-3, 6e-3, 8e-3, 10e-3] # A dictionary containing user specified input variables. # Unspecified parameters will be set to default values. parameter_dict = {\"size\": 5}  t_start = datetime.now() lattice_dict = sweep_parameter(parameter_to_vary, parameter_array, parameter_dict) t_end = datetime.now() print(f\"Time taken: {t_end - t_start}\") <pre>Time taken: 0:00:27.578930\n</pre> <p>We will now perform the same parameter sweep in parallel using the <code>multiprocessing</code> package. First, we import the <code>Pool</code> class which is used to make a pool of workers to which we can assign the tasks to be run in parallel. The number of workers is set by the <code>processes</code> argument when you instantiate the class. This argument can be ommitted, in which case the default behaviour is to create as many workers as your computer has CPUs.</p> In\u00a0[3]: Copied! <pre>from multiprocessing.pool import Pool\n</pre> from multiprocessing.pool import Pool <p>To run the code in parallel, we will be using the <code>pool.map</code> function (see the documentation here). Essentially, this is just a parallel version of the regular Python <code>map</code> function (see here) in which different elements of the iterable are passed to different workers in the pool. This means that we need to make an iterable of the input parameters, which is what we do in the cell below.</p> In\u00a0[4]: Copied! <pre>dict_list = []\nfor i, val in enumerate(parameter_array):\n    parameter_dict[parameter_to_vary] = val\n    dict_list.append(parameter_dict.copy())\n</pre> dict_list = [] for i, val in enumerate(parameter_array):     parameter_dict[parameter_to_vary] = val     dict_list.append(parameter_dict.copy()) <p>Now we run the parameter sweep in parallel. You can change the number of processes and see how this affects the time taken for the loop to complete. Note that, in this example, it is a bad idea to try and create more processes than your computer has cores as the code is limited by computation time, rather than I/O. In general, it is best to play around a bit with the number of processes you use to find out what is best for your particular situation.</p> In\u00a0[5]: Copied! <pre># The with statement runs the Pool object within a context manager.\n# This automatically frees up the resources used by the pool once all\n# processes have finished.\n# More information about context managers:\n# https://book.pythontips.com/en/latest/context_managers.html\nwith Pool(processes=3) as pool:\n    t_start = datetime.now()\n    # Assign the tasks to the workers in the pool\n    result = pool.map(single_parameter, dict_list)\nt_end = datetime.now()\nprint(f\"Time taken: {t_end - t_start}\")\n</pre> # The with statement runs the Pool object within a context manager. # This automatically frees up the resources used by the pool once all # processes have finished. # More information about context managers: # https://book.pythontips.com/en/latest/context_managers.html with Pool(processes=3) as pool:     t_start = datetime.now()     # Assign the tasks to the workers in the pool     result = pool.map(single_parameter, dict_list) t_end = datetime.now() print(f\"Time taken: {t_end - t_start}\") <pre>Time taken: 0:00:19.546347\n</pre> <p>For the 5x5 lattice, parallelisation is not a very effective way to speed up the code as the time it takes for a typical desktop computer to create the pool of workers negates the time saved by running the parameter sweep in parallel. However, for a larger lattice, the time taken to solve the Hamiltonian far exceeds the time required to make the pool and so parallelisation significantly reduces the total runtime.</p>"},{"location":"07-Using-multiprocessing/#running-for-loops-in-parallel","title":"Running for Loops in Parallel\u00b6","text":""},{"location":"08_Using-seaborn/","title":"Using <code>seaborn</code> to make heatmaps","text":"<p>Here, we will look at the <code>plot_state_distribution</code> function in the <code>lattice_plots.py</code> file. The purpose of this function is to plot how the electron and hole probability densities are distributed across the lattice. This is useful as it allows you to visualise how delocalised the eigenstates are. To see how this function can be used, we show an example of its output below (see also the final cell of the WorkedExample jupyter notebook):</p> <p></p> <p>A heatmap provides a convenient way to represent the electron and hole probability distributions for a 2D lattice. Although it is possible to make a heatmap using <code>matplotlib</code>, the <code>seaborn</code> package provides an alternative method and the plots generated tend to look nicer for less effort than those obtained using <code>matplotlib</code>. Consequently, we will discuss the <code>seaborn</code> package here. </p> <p>We start by exploring how the <code>plot_state_distribution</code> function works under the hood using <code>seaborn</code>. The following snippet shows a summary of the code, annotating what each part is doing:</p> <p></p> <p>We will now look at some of the arguemnts more closely, and you can find out more using the documentation. The arguments which were particularly useful to make this plot look visually appealing were: * <code>robust</code> - By default, seaborn maps the maximum and minimum colours of the heatmap you choose to the maximum and minimum values of the data being plotted. When plotting the occupation of the lattice sites, this behaviour is not desirable as most of the lattice sites have a very low occupation probaility and we want those sites to appear white, not light red/blue. Setting <code>robust=True</code> means that the colormap range is set by the data's interquartile range and so this issue is avoided. * <code>ax</code>- This argument tells seaborn on which axis to plot the heatmap, overruling the default behaviour of using the current axis. It is useful here as we want to show the electron and hole probability distributions on different plots so that both can be seen clearly.  * <code>square</code> - As we are simulating a square lattice, we would like the heatmap to reflect this. By setting <code>square=True</code>, seabron automatically sets the axes aspect to \"equal\" so that each cell in the heatmap is sqaure-shaped. </p>"},{"location":"A1_EnergyDependentRecombinationRates/","title":"Calculation of Recombination Rates Using Generalised Marcus-Levich-Jortner","text":"<p>If the <code>const_recombination</code> argument of the <code>Parameters</code> class is set to False, the eigenstate's decay rates are calculated using a version of Fermi's Golden Rule which has been adapted to describe organic molecules (the model is called generalised Marcus-Levich-Jortner and further detail can be found in references 1-3). In this framework, the decay rates are given by:</p> <p> </p> <p>where  refers to the Franck-Condon weighted density of states, which describes the wavefunction overlap between vibronic modes of the excited and ground states (see Figure One and reference 4). For non-radiative recombination, this is evaluated at  and is given by</p> <p> </p> <p>for a transition from an excited state to the ground state. It is defined in terms of the energy of the excited state with respect to the ground state (), the reorganization energy of thermally occupied low frequency phonon-modes coupled with the transition (), and the Huang-Rhys factor () of an effective high energy mode of energy  (). These parameters are related to the properties of the system's spectral density function, which was described in the file Introduction, as follows: the total reorgansition energy of the spectral density function is the sum of and  and  is given by the value of the spectral density's <code>e_peak</code> parameter. </p> <p></p> <p>Figure One: Illustration of the Franck-Condon Principle. The  () potential well represents the energy level of the ground (first excited) state as a function of the nuclear coordinate, . The potential energy of the electronic state is modelled as having a quadratic depenence on , giving rise to a series of vibronic modes for each electronic energy level. The number of phonons associated with a given vibronic mode is indicated by the quantum number . The rate at which the first excited state decays to the ground state will depend upon the wavefunction overlap of the vibronic part of both wavefunctions. Picture credit: By Samoza, CC BY-SA 3.0, Wikipedia. </p> <p>The overlap of the wavefunctions associated with the vibronic modes is approximated using the generalized Laguerre polynomials of degree , . The number of ground state and excited state phonon modes ( and , respectively) can generally be truncated depending on the temperature and energy of the effective mode. Phonon states are considered to be in thermal equilibrium and their occupation is normalised using the canonical partition function</p> <p> </p> <p>The coupling of the (delocalised) eigenstate to the ground state is calulated using:</p> <p> </p> <p>where  is the coupling of excitonic basis elements. </p> <p>Additionally, the model takes into account the fact that delocalised excited states typically have longer lifetimes than more locailised ones. To do this, the reorganisation energies are reduced by a factor of the inverse participation ratio assoicated with the relevant state (, see description in the file CalculatingEigenstateProperties) :</p> <p> </p> <p>where  can be either  or . </p>"},{"location":"A1_EnergyDependentRecombinationRates/#references","title":"References","text":"<ol> <li>Sumi, H. Theory on Rates of Excitation-Energy Transfer between Molecular Aggregates through Distributed Transition Dipoles with Application to the Antenna System in Bacterial Photosynthesis. J. Phys. Chem. B 103, 252\u2013260 (1999).</li> <li>B. Taylor, N. &amp; Kassal, I. Generalised Marcus theory for multi-molecular delocalised charge transfer. Chemical Science 9, 2942\u20132951 (2018).</li> <li>Azzouzi, M., Yan, J., Kirchartz, T. et al. Nonradiative Voltage Losses in Bulk-Heterojunction Organic Photovoltaics. Phys. Rev. X 8, 031055 (2018).</li> <li>Chemistry LibreTexts.</li> </ol>"},{"location":"A2_SpectralDensityFunctional/","title":"Description of the Spectral Density Function","text":"<p>We define the spectral density function of the basis states using the following functional, previously employed by Renger et al.  </p> <p> </p> <p>This function is shown in the Introduction. The parameter  is chosen such that the maximum of  occurs at  = 0.16 eV, a typical energy for common intra-molecular vibrations in conjugated organic molecules, such as C-C stretching bonds. We use a high value of  ( = 15) to ensure that  rapidly decays to zero for  to reflect the fact that organic molecules do not couple strongly to phonon modes with energies greater than ~ 0.2 eV (1600 cm ). Lastly,  is calculated so that the following normalisation condition is satisfied</p> <p> </p> <p>where  is the total input reorganisation energy of the basis states. </p>"},{"location":"A2_SpectralDensityFunctional/#references","title":"References","text":"<ol> <li>Renger, T. &amp; Marcus, R. A. On the relation of protein dynamics and exciton relaxation in pigment\u2013protein complexes: An estimation of the spectral density and a theory for the calculation of optical spectra. The Journal of Chemical Physics 116, 9997\u201310019 (2002).</li> </ol>"},{"location":"A3-SpeedingUp-for-Loops-ExampleCode/","title":"Speeding Up for Loops: Example Code","text":"<p>Run the cell below to import the necessary pacakges for this notebook.</p> In\u00a0[1]: Copied! <pre>import warnings\nfrom datetime import datetime\n\nfrom lattice_hamiltonian.lattice import Lattice, Parameters\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n</pre> import warnings from datetime import datetime  from lattice_hamiltonian.lattice import Lattice, Parameters  warnings.filterwarnings(\"ignore\", category=RuntimeWarning) <p>In the cell below, we implement a calculation of the rate matrix using an element-by-element <code>for</code> loop. Each iteration of the <code>for</code> loop corresponds to a calculation of the rate of population transfer between a pair of eigenstates. In each iteration, we need to take the scalar product of two $(\\mathrm{size})^{4} \\times 1$ vectors and calculate the value of the correlation function at the energy corresponding to the difference in energy of the two eigenstates.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom naive_correlation_function import correlation_function_real_part_single\n\nfrom lattice_hamiltonian.redfield import norm_spectral_density\n\n\ndef rates_mat_naive(lattice, params):\n    \"\"\"Calculate rates of population transfer between eigenstates.\n\n    Rates are calculates using Redfield theory within the secular approximation as\n    in Marcus and Renger 2002 &amp; Quantum biology revisited 2020.\n\n    Args:\n    lattice: An instance of the lattice class for which the eigenstates have\n    been found.\n    params: An instance of the parameters class.\n    \"\"\"\n    num_state = len(lattice.states)\n    rates_mat = np.zeros((num_state, num_state))\n    En_eV = lattice.states.energies\n    occupation_probability = lattice.states.occupation_probability\n    lambda_total = norm_spectral_density(\n        params.lambda_outer + params.lambda_inner, params.e_peak\n    )\n    for index_i in range(len(lattice.states)):\n        for index_j in range(index_i + 1, len(lattice.states)):\n            w = En_eV[index_j] - En_eV[index_i]\n            gamma = occupation_probability[index_j] @ occupation_probability[index_i]\n            Cw_Re = correlation_function_real_part_single(\n                w, lambda_total, params.e_peak, params.kT\n            )\n            rates_mat[index_j, index_i] = gamma * Cw_Re\n            # Energetically uphill and downhill rates are related by a Boltzmann factor.\n            rates_mat[index_i, index_j] = rates_mat[index_j, index_i] * np.exp(\n                -w / params.kT\n            )\n    return rates_mat\n</pre> import numpy as np from naive_correlation_function import correlation_function_real_part_single  from lattice_hamiltonian.redfield import norm_spectral_density   def rates_mat_naive(lattice, params):     \"\"\"Calculate rates of population transfer between eigenstates.      Rates are calculates using Redfield theory within the secular approximation as     in Marcus and Renger 2002 &amp; Quantum biology revisited 2020.      Args:     lattice: An instance of the lattice class for which the eigenstates have     been found.     params: An instance of the parameters class.     \"\"\"     num_state = len(lattice.states)     rates_mat = np.zeros((num_state, num_state))     En_eV = lattice.states.energies     occupation_probability = lattice.states.occupation_probability     lambda_total = norm_spectral_density(         params.lambda_outer + params.lambda_inner, params.e_peak     )     for index_i in range(len(lattice.states)):         for index_j in range(index_i + 1, len(lattice.states)):             w = En_eV[index_j] - En_eV[index_i]             gamma = occupation_probability[index_j] @ occupation_probability[index_i]             Cw_Re = correlation_function_real_part_single(                 w, lambda_total, params.e_peak, params.kT             )             rates_mat[index_j, index_i] = gamma * Cw_Re             # Energetically uphill and downhill rates are related by a Boltzmann factor.             rates_mat[index_i, index_j] = rates_mat[index_j, index_i] * np.exp(                 -w / params.kT             )     return rates_mat <p>Run the next cell to create a dictionary containing a series of lattices with different sizes (as described by the <code>sizes</code> list). This can take a little while as we need to find the eigenstates of each lattice.</p> In\u00a0[3]: Copied! <pre>sizes = [4, 5, 6, 7, 8]\nlattice_dict = {}\nfor i, s in enumerate(sizes):\n    print(i)\n    spacing = 10\n    num_sites_coupled = 1.45\n    size = s\n\n    j0 = 1.5\n    r0j = 0.1\n\n    params = Parameters(\n        temp=300,\n        e_peak=0.16,\n        lambda_inner=0.202,\n        lambda_outer=0.048,\n        j0=j0,\n        r0j=r0j * spacing,\n        e_singlet=1.4,\n        const_recombination=True,\n    )\n\n    lattice = Lattice()\n    lattice.generate_uniform(\n        size=size,\n        HOMO=0,\n        LUMO=1.8,\n        dist_sites=spacing,\n        min_dist_near_neighbour=(num_sites_coupled * spacing) + 0.01,\n        t0_homo=0.005,\n        t0_lumo=0.005,\n        d0=0.005,\n        r0d=0.1 * spacing,\n        v_ex=0,\n        krec_ex=1e9,\n        const_recombination=True,\n    )\n\n    lattice.build_ham(\n        params,\n        F=[0, 0, 0],\n        min_dist_near_neighbour=(num_sites_coupled * spacing) + 0.01,\n        disorder_site_ene=0.05,\n        random_seed=42,\n    )\n\n    # calculate the eigenstates of the lattice\n    lattice.states_from_ham(params, max_energy_diff=1.5)\n    lattice_dict[i] = lattice\n</pre> sizes = [4, 5, 6, 7, 8] lattice_dict = {} for i, s in enumerate(sizes):     print(i)     spacing = 10     num_sites_coupled = 1.45     size = s      j0 = 1.5     r0j = 0.1      params = Parameters(         temp=300,         e_peak=0.16,         lambda_inner=0.202,         lambda_outer=0.048,         j0=j0,         r0j=r0j * spacing,         e_singlet=1.4,         const_recombination=True,     )      lattice = Lattice()     lattice.generate_uniform(         size=size,         HOMO=0,         LUMO=1.8,         dist_sites=spacing,         min_dist_near_neighbour=(num_sites_coupled * spacing) + 0.01,         t0_homo=0.005,         t0_lumo=0.005,         d0=0.005,         r0d=0.1 * spacing,         v_ex=0,         krec_ex=1e9,         const_recombination=True,     )      lattice.build_ham(         params,         F=[0, 0, 0],         min_dist_near_neighbour=(num_sites_coupled * spacing) + 0.01,         disorder_site_ene=0.05,         random_seed=42,     )      # calculate the eigenstates of the lattice     lattice.states_from_ham(params, max_energy_diff=1.5)     lattice_dict[i] = lattice <pre>0\n1\n2\n3\n4\n</pre> <p>In the next cell, we use our element-by-element implementation of the code (defined above) to calculate the rate matrix for each lattice in <code>lattice_dict</code>.</p> In\u00a0[4]: Copied! <pre># calculate the rates of transitions between states using the naive implementation\nnaive_times = np.zeros(len(sizes))\nfor i in range(len(sizes)):\n    print(i)\n    t1 = datetime.now()\n    rates = rates_mat_naive(lattice_dict[i], params)\n    t2 = datetime.now()\n    naive_times[i] = (t2 - t1).total_seconds()\n</pre> # calculate the rates of transitions between states using the naive implementation naive_times = np.zeros(len(sizes)) for i in range(len(sizes)):     print(i)     t1 = datetime.now()     rates = rates_mat_naive(lattice_dict[i], params)     t2 = datetime.now()     naive_times[i] = (t2 - t1).total_seconds() <pre>0\n1\n2\n3\n4\n</pre> <p>And now we calculate the rates using the vectorized version of the code...</p> In\u00a0[5]: Copied! <pre># calculate the rates of transitions between states using vectorized code\nvectorized_times = np.zeros(len(sizes))\nfor i in range(len(sizes)):\n    print(i)\n    t3 = datetime.now()\n    lattice_dict[i].get_rate_mat(params)\n    t4 = datetime.now()\n    vectorized_times[i] = (t4 - t3).total_seconds()\n</pre> # calculate the rates of transitions between states using vectorized code vectorized_times = np.zeros(len(sizes)) for i in range(len(sizes)):     print(i)     t3 = datetime.now()     lattice_dict[i].get_rate_mat(params)     t4 = datetime.now()     vectorized_times[i] = (t4 - t3).total_seconds() <pre>0\n1\n2\n3\n4\n</pre> <p>Finally, we plot the difference in the times taken for the two versions of the code to calculate the rate matrix. By using a log-log, we can see that vectorizing the code substantialy speeds up the calculation but, to a good approximation, it does not change how the total time taken scales with the lattice size.</p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(6, 4), facecolor=\"white\", dpi=300)\nax = fig.add_axes([0.15, 0.25, 0.5, 0.7])\nax.loglog(sizes, naive_times, ls=\"--\", marker=\"s\", label=\"One-by-one\")\nax.loglog(sizes, vectorized_times, ls=\"--\", marker=\"o\", label=\"Vectorized\")\n\nax.set_xlabel(\"Lattice Size\", fontsize=18)\nax.set_xticks(sizes,['4','5','6','7','8'])\nax.set_ylabel(\"Time Taken to Calculate\\n Rate Matrix (s)\", fontsize=18)\nax.tick_params(direction=\"in\", right=True, top=True, labelsize=12)\nax.legend(fontsize=12)\n</pre> import matplotlib.pyplot as plt  fig = plt.figure(figsize=(6, 4), facecolor=\"white\", dpi=300) ax = fig.add_axes([0.15, 0.25, 0.5, 0.7]) ax.loglog(sizes, naive_times, ls=\"--\", marker=\"s\", label=\"One-by-one\") ax.loglog(sizes, vectorized_times, ls=\"--\", marker=\"o\", label=\"Vectorized\")  ax.set_xlabel(\"Lattice Size\", fontsize=18) ax.set_xticks(sizes,['4','5','6','7','8']) ax.set_ylabel(\"Time Taken to Calculate\\n Rate Matrix (s)\", fontsize=18) ax.tick_params(direction=\"in\", right=True, top=True, labelsize=12) ax.legend(fontsize=12) Out[7]: <pre>&lt;matplotlib.legend.Legend at 0x1445c5fc320&gt;</pre>"},{"location":"A3-SpeedingUp-for-Loops-ExampleCode/#speeding-up-for-loops-example-code","title":"Speeding Up for Loops: Example Code\u00b6","text":""},{"location":"naive_correlation_function/","title":"Naive correlation function","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Calculate rates of population transfer between eigenstates.\n\nRates are calculates using Redfield theory within the secular approximation as in\nMarcus and Renger 2002 &amp; Quantum biology revisited 2020.\n\"\"\"\n</pre> \"\"\"Calculate rates of population transfer between eigenstates.  Rates are calculates using Redfield theory within the secular approximation as in Marcus and Renger 2002 &amp; Quantum biology revisited 2020. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport scipy.constants as const\n</pre> import numpy as np import scipy.constants as const In\u00a0[\u00a0]: Copied! <pre>def correlation_function_real_part_single(\n    w: float, lambda_total: float, e_peak: float, kT: float\n) -&gt; float:\n    \"\"\"Calculates the real part of the correlation function of the lattice.\n\n    Args:\n        w: The energy difference between the two eigenstates of interest.\n        lambda_total: The total reorganisation energy associated with each molecule.\n            Defined to be the sum of the outer and inner reorganisation energies. Units\n            are eV.\n        e_peak: The energy of the peak of the spectral density function in eV.\n            Typically 0.16 eV for organic molecules.\n        kT: The typical thermal energy of the lattice in eV.\n\n    Returns:\n        float: The value of the correlation function evaluted at w.\n    \"\"\"\n    C_w = (\n        (1 + bose_einstein_occupancy_single(w, kT))\n        * (\n            (spectral_density_single(w, lambda_total, e_peak))\n            - (spectral_density_single(-w, lambda_total, e_peak))\n        )\n    ) / (const.hbar / const.e)\n    return 2 * np.pi * C_w\n</pre> def correlation_function_real_part_single(     w: float, lambda_total: float, e_peak: float, kT: float ) -&gt; float:     \"\"\"Calculates the real part of the correlation function of the lattice.      Args:         w: The energy difference between the two eigenstates of interest.         lambda_total: The total reorganisation energy associated with each molecule.             Defined to be the sum of the outer and inner reorganisation energies. Units             are eV.         e_peak: The energy of the peak of the spectral density function in eV.             Typically 0.16 eV for organic molecules.         kT: The typical thermal energy of the lattice in eV.      Returns:         float: The value of the correlation function evaluted at w.     \"\"\"     C_w = (         (1 + bose_einstein_occupancy_single(w, kT))         * (             (spectral_density_single(w, lambda_total, e_peak))             - (spectral_density_single(-w, lambda_total, e_peak))         )     ) / (const.hbar / const.e)     return 2 * np.pi * C_w In\u00a0[\u00a0]: Copied! <pre>def bose_einstein_occupancy_single(w: float, kT: float) -&gt; float:\n    \"\"\"Bose-Einstein occupancy function.\n\n    Args:\n        w: The energy difference between the two eigenstates of interest.\n        kT: The typical thermal energy of the lattice in eV.\n\n    Returns:\n        n: The value of the Bose-Einstein occupation function for the input w and kT.\n    \"\"\"\n    if w == 0:\n        n = np.inf\n    else:\n        n = 1 / (np.exp(w / kT) - 1)\n    return n\n</pre> def bose_einstein_occupancy_single(w: float, kT: float) -&gt; float:     \"\"\"Bose-Einstein occupancy function.      Args:         w: The energy difference between the two eigenstates of interest.         kT: The typical thermal energy of the lattice in eV.      Returns:         n: The value of the Bose-Einstein occupation function for the input w and kT.     \"\"\"     if w == 0:         n = np.inf     else:         n = 1 / (np.exp(w / kT) - 1)     return n In\u00a0[\u00a0]: Copied! <pre>def spectral_density_single(\n    w: float,\n    lambda_total: float,\n    e_peak: float,\n    n: int = 15,\n) -&gt; float:\n    \"\"\"The spectral density function of the molecules' phonon modes.\n\n    Args:\n        w: The energy difference between the two eigenstates of interest.\n        lambda_total: The total reorganisation energy associated with each molecule.\n            Defined to be the sum of the outer and inner reorganisation energies.\n            Units are eV.\n        e_peak: The energy of the peak of the spectral density function in eV.\n            Typically 0.16 eV for organic molecules.\n        n: Parameter determining how rapidly the spectral denisty function decays for\n            w &gt; e_peak. Default values is 15.\n\n    Returns:\n        J_w: The value of the spectral density evaluted at w.\n    \"\"\"\n    if w &gt;= 0:\n        a = e_peak / (3 / n) ** (1 / n)\n        J_w = lambda_total * w**3 * np.exp((-w / a) ** n)\n    else:\n        J_w = 0\n    return J_w\n</pre> def spectral_density_single(     w: float,     lambda_total: float,     e_peak: float,     n: int = 15, ) -&gt; float:     \"\"\"The spectral density function of the molecules' phonon modes.      Args:         w: The energy difference between the two eigenstates of interest.         lambda_total: The total reorganisation energy associated with each molecule.             Defined to be the sum of the outer and inner reorganisation energies.             Units are eV.         e_peak: The energy of the peak of the spectral density function in eV.             Typically 0.16 eV for organic molecules.         n: Parameter determining how rapidly the spectral denisty function decays for             w &gt; e_peak. Default values is 15.      Returns:         J_w: The value of the spectral density evaluted at w.     \"\"\"     if w &gt;= 0:         a = e_peak / (3 / n) ** (1 / n)         J_w = lambda_total * w**3 * np.exp((-w / a) ** n)     else:         J_w = 0     return J_w"}]}